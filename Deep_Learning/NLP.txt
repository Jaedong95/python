''' Text data preprocessing '''
# 1. 영어
## 1) 단어 토큰화  
from nltk.tokenize import word_tokenize   # tokenizer_01 
from nltk.tokenize import WordPunctTokenizer   # tokenizer_02
from nltk.tokenize import TreebankWordTokenizer   # tokenizer_03 
from tensorflow.keras.preprocessing.text import text_to_word_sequence   # tokenizer_04

sentence = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."
print('단어 토큰화1 :',word_tokenize(sentence))
print('단어 토큰화2 :',WordPunctTokenizer().tokenize(sentence))
print('단어 토큰화3 :',text_to_word_sequence(sentence))

tokenizer = TreebankWordTokenizer()

text = "Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own."
print('트리뱅크 워드토크나이저 :',tokenizer.tokenize(text))

## 2) 문장 토큰화 
from nltk.tokenize import sent_tokenize

text = """His barber kept his word. But keeping such a huge secret to himself was driving him crazy.
          Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in 
          the midst of some reeds. He looked about, to make sure no one was near."""
print('문장 토큰화1 :',sent_tokenize(text))


## 3) 정제  - 단어 길이 수가 1-2 이하인 영어 단어 제외
import re

text = "I was wondering if anyone out there could enlighten me on this car."

shortword = re.compile(r'\W*\b\w{1,2}\b')  # 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제
print(shortword.sub('', text))


## 4) 표제어 추출 
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']

print('표제어 추출 전 :',words)
print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])
print('품사 지정하여 표제어 추출: ', lemmatizer.lemmatize('dies', 'v'))


## 5) 어간 추출 
from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()
lancaster_stemmer = LancasterStemmer()

sentence = """This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names
		   and heights and soundings--with the single exception of the red crosses and the written notes."""
tokenized_sentence = word_tokenize(sentence)

print('어간 추출 전 :', tokenized_sentence)
print('포터 스테머의 어간 추출 후 :', [stemmer.stem(word) for word in tokenized_sentence])
print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in tokenized_sentence])


## 6) 불용어 
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
from konlpy.tag import Okt

stop_words_list = stopwords.words('english')

print('불용어 개수 :', len(stop_words_list))   
print('불용어 10개 출력 :',stop_words_list[:10])

text = "Family is not an important thing. It's everything."
stop_words = set(stopwords.words('english')) 

word_tokens = word_tokenize(text)

result = []
for word in word_tokens: 
    if word not in stop_words: 
        result.append(word) 

print('불용어 제거 전 :',word_tokens) 
print('불용어 제거 후 :',result)


## 7) 정수 인코딩 
### 컴퓨터는 텍스트보다 숫자를 더 잘 처리할 수 있기 때문에, 각 단어를 고유한 정수에 매핑하는 전처리 작업이 필요

### 7-1) Dictionary 자료형을 이용한 정수 인코딩
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

raw_text = """A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret!
		   The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word.
		   His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the 
		   barber crazy. the barber went up a huge mountain."""

sentences = sent_tokenize(raw_text)   # 문장 단위 토큰화 
print(sentences)

vocab = {}   # dictionary 자료형 선언 
preprocessed_sentences = []
stop_words = set(stopwords.words('english'))   # 불용어 생성 

for sentence in sentences:   # 각 문장에 대해 
    tokenized_sentence = word_tokenize(sentence)   # 단어 토큰화
    result = []

    for word in tokenized_sentence:    # 각 단어에 대해
        word = word.lower() # 모든 단어를 소문자로 변환
        if word not in stop_words:   # 불용어 제거 
            if len(word) > 2:    # 단어 길이가 2이하인 단어들 제외
                result.append(word)
                if word not in vocab:
                    vocab[word] = 0 
                vocab[word] += 1
    preprocessed_sentences.append(result) 

print(preprocessed_sentences)    # [['barber', 'person'], ['barber', 'good', 'person'] ... ]
print('단어 집합: ', vocab)   # vocab에는 각 단어의 빈도수가 기록되어 있음 

vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)   # 빈도수가 높은 순서대로 정렬 

word_to_index = {}
i = 0
for (word, frequency) in vocab_sorted :   # 높은 빈도를 가진 단어일수록 낮은 정수를 부여함 (정수는 1부터 시작)
    if frequency > 1 : # 빈도수가 작은 단어는 제외
        i = i + 1
        word_to_index[word] = i

print(word_to_index)

vocab_size = 5   # 빈도수가 가장 높은 5개의 단어만 출력하도록 설정 
words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]

for w in words_frequency:
    del word_to_index[w]   # 해당 단어에 대한 인덱스 정보를 삭제
print(word_to_index)   # 빈도수가 가장 높은 상위 5개 단어만 저장됨

word_to_index['OOV'] = len(word_to_index) + 1   # 인덱스 값이 5 이상인 단어들을 정수로 바꿀 때 'OOV' 값에 매핑
print(word_to_index)   # {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}

encoded_sentences = []
for sentence in preprocessed_sentences:   # 전처리 완료된 문장들에 대해 
    encoded_sentence = []
    for word in sentence:   
        try:
            encoded_sentence.append(word_to_index[word])  # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴
        except KeyError:
            encoded_sentence.append(word_to_index['OOV'])   # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴
    encoded_sentences.append(encoded_sentence)
print(encoded_sentences)
		   

# 2. 한글 
## 1) 단어 토큰화  
from konlpy.tag import Okt
from konlpy.tag import Kkma

okt = Okt()   # 트위터 형태소 분석기 
kkma = Kkma()   # 꼬꼬마 형태소 분석기 

print('OKT 형태소 분석 :', okt.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('OKT 품사 태깅 :', okt.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('OKT 명사 추출 :', okt.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))

## 2) 문장 토큰화  
!pip install kss

import kss

text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'
print('한국어 문장 토큰화 :',kss.split_sentences(text))


## 3) 불용어 - 한국어의 경우 사용자가 직접 불용어 사전을 만드는 경우가 많음 
from konlpy.tag import Okt

okt = Okt()

example = "고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지."
stop_words = "를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는"

stop_words = set(stop_words.split(' '))
word_tokens = okt.morphs(example)

result = [word for word in word_tokens if not word in stop_words]

print('불용어 제거 전 :',word_tokens) 
print('불용어 제거 후 :',result)



''' BoW '''
# BoW란 ? 단어들의 순서는 전혀 고려하지 않고 단어들의 출현 빈도에만 집중하는 
#         텍스트 데이터의 수치화 표현 방법 
from konlpy.tag import Okt

okt = Okt()

def build_bag_of_words(document):
  '''
  document를 입력으로 받아 (단어-인덱스 매핑값, bow) 생성해서 반환하는 함수 
  '''
  document = document.replace('.', '')   # 온점 제거 
  tokenzied_document = okt.morphs(document)   # 형태소 분석 
  
  word_to_index = {}   # dictionary
  bow = []   # list 
  
  for word in tokenized_document:
    if word not in word_to_index.keys():
      word_to_index[word] = len(word_to_index)
      # bow에 전부 기본값 1을 넣음 
      bow.insert(len(word_to_index) - 1, 1) 
    else:
      index = word_to_index.get(word)  # 재등장하는 단어의 인덱스
      # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더함 
      bow[index] = bow[index] + 1 
  return word_to_index, bow 

# bow examples(1)  - python
doc = "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다."
vocab, bow = build_bag_of_words(doc)
print('vocabulary: ', vocab)
print('bag of words vector: ', bow)

# bow examples(2)  - scikit-learn 
from sklearn.feature_extraction.text import CountVectorizer

corpus = ['you know I want your love. because I love you.']
## CountVectorizer(): 단어의 빈도를 Count하여 Vector로 생성함 
vector = CountVectorizer()   

print('vocabulary :',vector.vocabulary_)   # 각 단어의 인덱스가 어떻게 부여되었는지를 출력
# 코퍼스로부터 각 단어의 빈도수를 기록
print('bag of words vector :', vector.fit_transform(corpus).toarray()) 


''' TF-IDF ''' 
# 문서: d, 단어: t, 문서의 총 개수: n 
# df(t): 특정 단어 t가 등장한 문서 수 

def tf(t, d):
  ''' 
  특정 문서 d에서의 특정 단어 t의 등장 횟수 반환 
  '''
  return d.count(t)
 
def idf(t):
  '''
  df(t)의 역수 값(log를 취해 가중치 비중 감소)을 반환
  idf 함수는 여러 문서에서 등장한 단어의 가중치를 낮추는 역할 수행  
  
  전체 문서의 수 N = 3, df = 2인 경우, log(N/(df+1)) = 0이 되어 
  가중치의 역할을 수행하지 못하기 때문에, 실무에서는 이 수식을 조정해서 사용함
  '''
  df = 0
  for doc in docs:
    df += t in doc
  return log(N/(df+1))
  
def tfidf(t, d):
  '''
  tf x idf를 계산한 값 
  특정 문서에 등장하는 빈도수가 높고, 다른 문서에 등장하는 빈도수가 낮을수록 값이 커짐 
  '''
  return tf(t,d)* idf(t)
  

# tf-idf examples (1) - python
import pandas as pd 
from math import log 

docs = [
  '먹고 싶은 사과',
  '먹고 싶은 바나나',
  '길고 노란 바나나 바나나',
  '저는 과일이 좋아요'
]

## 문서(문자열)에 등장하는 단어(공백으로 구분된)들에 대해 중복을 제거하여 집합으로 생성 (단어집)
vocab = list(set(w for doc in docs for w in doc.split()))
vocab.sort() 

n = len(docs)   # 총 문서의 수 

## tf 구현 
result = [] 

for i in range(n):   # 각 문서에 대해 
  result.append([]) 
  d = docs[i]  # 문서 지정 
  for j in range(len(vocab)):  # 단어집의 개수만큼 반복
    t = vocab[j]  # 단어 지정
    result[-1].append(tf(t, d))   # tf 값 계산하여 배열의 마지막에 추가 
    
tf_dataFrame = pd.DataFrame(result, columns = vocab)  # DTM
 
## idf 구현 
result = []
 
for j in range(len(vocab)): 
  t = vocab[j] 
  result.append(idf(t)) 
   
if_dataFrame = pd.DataFrame(result, index=vocab, columns=["IDF"])
  
## TF-IDF 행렬 구현 
result = [] 
for i in range(n):   # 모든 문서에 대해
  result.append([])
  d = docs[i]
  for j in range(len(vocab)):
    t = vocab[j]
    result[-1].append(tfidf(t, d))
    
tfidf_dataFrame = pd.DataFrame(result, columns=vocab)

# tf-idf examples (2)  - scikit-learn
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.feature_extraction.text import TfidfVectorizer 

corpus = [
  'you know I want your love', 
  'I like you',
  'what should I do'
]

vector = CountVectorizer()

# 코퍼스로부터 각 단어의 빈도 수 기록  - DTM 생성 
print(vector.fit_transform(corpus).toarray())

# 각 단어와 매핑된 인덱스 출력 
print(vector.vocabulary_) 

## scikit-learn의 tfidfVectorizer는 IDF의 분자항에 1을 더해주고, 로그항에 1을 더해주고
## TF-IDF에 L2 regularization 적용하여 tf-idf 값을 계산함 
tfidf_v = TfidfVectorizer().fit(corpus)

print(tfidf_v.transform(corpus).toarray())
print(tfidf_v.vocabulary_)


''' 유사도 계산 ''' 
# 01. 코사인 유사도
from numpy import dot 
from numpy.linalg import norm  
import numpy as np

def cos_sim(A, B):
  '''
  numpy를 이용해 A, B 문서 간 코사인 유사도 계산하는 함수 
  '''
  return dot(A, B) / (norm(A) * norm(B))

doc1 = np.array([0, 1, 1, 1])
doc2 = np.array([1, 0, 1, 1])
doc3 = np.array([2, 0, 2, 2]) 

print('문서 1과 문서2의 유사도 :', cos_sim(doc1, doc2))
print('문서 1과 문서3의 유사도 :', cos_sim(doc1, doc3))
print('문서 2와 문서3의 유사도 :', cos_sim(doc2, doc3))

# 02. 유클리드 거리
import numpy as np

def dist(A, B):
  '''
  numpy를 이용해 A, B 문서 간 유클리드 거리를 계산하는 함수 
  '''
  return np.sqrt(np.sum((A-B) ** 2))
  
doc1 = np.array((2, 3, 0, 1)) 
doc2 = np.array((1, 2, 3, 1))
doc3 = np.array((2, 1, 2, 2)) 
doc4 = np.array((1, 1, 0, 1)) 

print('문서 1과 문서4의 거리 :', dist(doc1, doc2))
print('문서 2와 문서4의 거리 :', dist(doc1, doc3))
print('문서 3과 문서4의 거리 :', dist(doc2, doc3))

# 03. 자카드 유사도
doc1 = "apple banana everyone like likely watch card holder"
doc2 = "apple banana coupon passport love you"

## 토큰화
tokenized_doc1 = doc1.split()
tokenized_doc2 = doc2.split()

print('문서 1: ', tokenized_doc1)
print('문서 2: ', tokenized_doc2)

union_AB = set(tokenized_doc1).union(set(tokenzied_doc2))
print('문서1과 문서2의 합집합: ', union_AB)

intersect_AB = set(tokenized_doc1).intersection(set(tokenized_doc2))
print('문서1과 문서2의 교집합: ', intersect_AB)

print('자카드 유사도: ', len(intersect_AB)/len(union_AB))


# 04. 레벤슈타인 거리 
## 레벤슈타인 거리란 ? 두 개의 문장을 2차원 배열로 나타내어 각 문장의 문장들을 
##				비교하며 몇 개의 글자가 다른지 구분하는 방법 
def leven(A, B):
  '''
  두 문장 A, B를 입력으로 받아 서로 몇 개의 단어가 다른지 반환하는 함수 
  1. [문장길이 + 1][문장길이 + 1] 만큼의 2차원 배열을 생성
  2. [0][n], [n][0]을 모두 문장 길이만큼 초기화
  3. 문장 A의 첫 단어와 문장 B의 첫 단어가 같으면 cost를 0으로 주고, [i][j] = [i-1][j-1] + cost
  4. 문장 A의 첫 단어와 문장 B의 두번째 단어가 다르면 두 번째 단어를 제거, [i][j] = [i-1][j] + 1 
  5. 문장 A의 두번째 단어와 문장 B의 첫 단어가 다르면 
  6. [i][j]는 3-5 경우 중 가장 값이 작은 것을 넣어줌 
  7. [배열 크기 - 1][배열 크기 - 1]의 값을 반환 (이 값이 서로 다른 단어의 개수에 해당)
  '''
  len_A = len(A) + 1 
  len_B = len(B) + 1 
  array = [[] for _ in range(len_A)]
  for i in range(len_A):
    array[i] = [0 for a in range(len_B)]
  for i in range(len_B):
    array[0][i] = i
  for i in range(len_A):
    array[i][0] = i
  
  cost = 0 
  for i in range(1, len_A):
    for j in range(1, len_B):
      if A[i-1] != B[j-1]:
        cost = 1
      else:
        cost = 0
      addNum = array[i-1][j] + 1  # 추가 
      minusNum = array[i][j-1] + 1   # 감소
      modiNum = array[i-1][j-1] + cost 
      minNum = min([addNum, minusNum, modiNum])
      array[i][j] = minNum 
  return array[len_A - 1][len_B - 1]

leven("가나다라", "가마바라")

samples = ["신발","신촌역","신천군","신천역","마곡역"]
result = sorted(samples, key = lambda x : leven("신촌역",x))

for i in result: 
  print(i, ":", lenven("신촌역", i))