''' CNN 훈련하기, 2022.02.22 '''
1. 합성곱 계층 
- 합성곱 계층에서 각 유닛은 비선형성과 결합된 필터들이라고 할 수 있음

$ from tensorflow.keras.layers import Conv2D
$
$ Conv2D(64, kernel_size=(3, 3), activation='relu', name='conv')

-> 해당 계층에는 64개의 개별 유닛이 있으며, 각 유닛은 3*3*3 꼴로 이루어진 필터라고 할 수 있음

- 32*32*3 형태의 입력 이미지 -> 'conv' layer -> 30 * 30 * 64 형태의 출력이 나옴

1) 합성곱 계층의 장점
- 32*32*3 형태의 입력 이미지  -> 3,072개의 입력 파라미터 
- 30*30*64 형태의 출력  -> 57,600개의 출력 파라미터 
- 이 둘을 완전 연결 계층을 사용해 연결할 경우, 해당 계층에는 176,947,200개의 훈련 가능한 파라미터가 나옴
- Bt 이 둘을 합성곱 계층을 사용하여 64개의 3*3*3 필터를 사용할 경우 1,792개의 파라미터에 대해 64개의 편향치만 추가 
     (훨씬 적은 파라미터)


* Convolution Parameter 개수 구하는 방법 
-> (filter width * filter height * input channel) * filter 개수 + filter 개수 (bias)

2) 국부적 연결성 
- 필터들의 크기가 고정되어 있음 -> 필터들은 인접 픽셀 간의 연결성에 초점을 맞춤 
- 이로써 필터들은 국부적 특징 (local features)을 가장 강하게 학습할 수 있음
- 비선형성을 지닌 계층 내의 다른 필터들과 결합하면서 점점 더 크고 복잡한 특징에 주의를 기울일 수 있음

3) 풀링 계층
- 합성공 계층이 추가될 때 합성곱 신경망의 차원을 줄이는데 사용 (과적합을 줄일 수 있음)
- 풀링 계층을 통해 좀 더 대표성을 지니는 특징을 잘 찾아낼 수 있음
- 일반적으로 행렬을 겹치지 않는 부분들로 나누고, 각 부분에서 최대값을 취함 (max pooling)

$ from tensorflow.keras.layers import MaxPooling2D
$
$ pool = MaxPooling2D(pool_size=(2, 2), name='pool')

* 합성곱 계층이나 풀링 계층 모두에서 패딩의 기본값은 패딩이 없음을 의미하며, padding='same' 옵션을 지정해 주어야 패딩이 적용됨

4) 배치 정규화
- 각 미니배치에 대해 각 비선형성 처리 전 또는 후에 평균이 0이고 단위 분산이 되도록 배치를 정규화 
- 배치 정규화를 통해 각 계층이 정규화된 입력을 학습할 수 있고, 좀 더 효율적인 학습이 가능해짐

$ from tensorflow.keras.layers import BatchNormalization
$ 
$ x = BatchNormalization(name='batch_norm')


2. 사용 데이터셋, CIFAR-10
- 10개 클래스로 분류되는 32*32 컬러 이미지 6만 장
- 각 클래스 당 이미지는 6,000장 
- 5만 개 이미지를 훈련용 집합, 5,000개 이미지를 검증용 집합, 5,000개 이미지를 테스트 집합으로 사용


3. 모델 설계
1) 입력
- 합성곱 신경망을 위한 입력 텐서 계층: (N, 32, 32, 3)

$ def build_network(num_gpu=1, input_shape=None):
$     inputs = Input(shape=input_shape, name='input')

2) 출력
- 클래스 예측을 출력하며, 예측 값은 0~9에 해당 
- softmax 사용 

$ output = Dense(10, activation='softmax', name='softmax')(d2)

3) 비용 함수
- 범주형 교차 엔트로피 사용

4) 계량 함수 
- 정확도 사용 


4. 계층 
1) 합성곱 계층
- 배치 정규화, 최대 풀링 및 두 개의 합성곱 계층 사용 

$ conv1 = Conv2D(64, kernel_size=(3, 3), activation='relu', name='conv_1')(inputs)
$ batch1 = BatchNormalization(name='batch_norm1')(conv1)
$ pool1 = MaxPooling2D(pool_size=(2, 2), name='pool_1')(batch1) 

-> conv2도 필터 수: 32개인 것을 제외하고는 동일 

2) 완전 연결 계층
- 두 번의 합성곱 계층과 풀링 계층을 거치면 출력 차원은 (N, 6, 6, 3) 형태가 됨
- 해당 값이 최종 출력 계층으로 가기 전에, 먼저 완전 연결 계층과 연결되야 함 

$ from tensorflow.keras.layers import Flatten, Dense, Dropout 
$
$ flatten = Flatten()(pool2)
$ fc1 = Dense(512, activation='relu', name='fc1')(flatten)
$ d1 = Dropout(rate=0.2, name='dropout1')(fc1) 

-> fc2도 뉴런 수 256개인 것을 제외하고는 동일
   flatten() 함수를 이용해 n*6*6*32 텐서를 n*1152 벡터로 평탄하게 만들고, 완전 연결 계층에 전달함


5. 다중 GPU 이용
$ from tensorflow.keras.utils import multi_gpu_model
$ 
$ model = Model(inputs=inputs, outputs=output)
$ model = multi_gpu_model(model, num_gpu)


6. 모델 훈련 
1) 모델 호출 및 컴파일
$ model = Model(inputs=inputs, outputs=output)
$
$ if num_gpu > 1:
$     model = multi_gpu_model(model, num_gpu)
$ model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

2) 모델 학습
$ model.fit(x=data['train_X'], y=data['train_y'], batch_size=32, epochs=200, validation_data=(data['val_X'], data['val_y']),
$           verbose=1, callbacks=callbacks)


7. 데이터 확대 (data augmentation)
- 이미지를 변형시킨 후 원본 이미지와 변형된 이미지를 모두 사용해 훈련하는 방법
- 수평/수직 뒤집기, 이동, 무작위 회전 등의 방법을 사용할 수 있음

1) ImageDataGenerator
- 케라스에서 제공하는 데이터 변형 클래스로, 모델을 훈련하는 동안 변형 코드를 제시하지 않아도 즉시 변형할 수 있도록 함 
- 변형을 코드화하고, 훈련 집합에 무작위로 적용하고, 저장하는 방법보다 훨씬 편리 

$ from tensorflow.keras.preprocessing.image import ImageDataGenerator
$ 
$ def create_datagen(train_X):
$     data_generator = ImageDataGenerator(rotation_range=20, width_shift_range=0.02, height_shift_range=0.02,
$                                         horizontal_flip=True)
$     data_generator.fit(train_X)
$
$     return data_generator

-> 이동, 회전, 수평 뒤집기를 모두 사용함 

# 학습
$ model.fit_generator(data_generator.flow(data['train_X'], data['train_y'], batch_size=32), 
$                     steps_per_epoch=len(data['train_X']) // 32, epochs=200,   # 32: image pixels 
$ 		      validation_data=(data['val_X'], data['val_y']), verbose=1, callbacks=callbacks)

-> ImageDataGenerator.flow() 함수를 호출할 때 마다 제공된 이미지에 무작위 변환을 적용해 새로운 훈련용 미니배치를 생성함
   steps_per_epoch 파라미터를 통해 훈련 집합으로부터 몇 번이든 표본을 바꿀 수 있으며, 매번 무작위 변형을 적용할 수 있음 


''' 전이 학습, 2022.02.22 '''
0. 이미지넷 데이터셋
- 최첨단 기술이라 생각할 수 있는 심층 신경망은 '이미지넷' 데이터셋으로 훈련하는 경우가 많음 
- 120만개 이미지를 1,000개 클래스로 분류하는 분류기


1. 전이 학습
- 수많은 계층이 있고 수백만 개 이미지로 훈련을 받은 최첨단 신경망 중 하나를 아주 적은 데이터만 사용하는 우리 문제에 적용할 수 있도록 도와줌
- 전이 학습을 하려면 다음 단계를 수행함 

1) 매우 복잡한 컴퓨터 비전 문제로 훈련을 받은 모델을 가져옴 (원본 정의역)
2) 신경망의 마지막 계층(softmax), 추가 완전 연결 계층들을 제거
3) 마지막 몇 개의 계층을 새로운 문제에 적합한 계층으로 바꿈 (대상 정의역)
4) 이미 훈련된 모든 계층을 동결해 모델 학습 중 가중치가 변경되지 않도록 설정
5) 대상 정의역 데이터를 이용해 신경망 훈련 

* 동결된 계층(frozen layer)의 일부 또는 전부를 선택적으로 풀어서 전체 신경망을 미세 조정(fine-tuning)하고 아주 느린 학습 속도로 다시 훈련하기도 함 


2. 전이 학습을 사용하는 대표적인 경우들 
1) 제한된 데이터
- 이미지가 최소 수천개의 학습용 이미지, 보통 1만 ~ 2만개의 이미지 정도 있는 경우 

2) 공통 문제 정의역
- 대상 정의역이 원본 정의역과 어느 정도 비슷한 경우


3. 원본 및 대상 정의역의 크기와 유사도의 영향
1) 데이터는 많으면 많을수록 좋다
- 구글에서는 3억 회의 관측치를 포함하는 내부 데이터셋으로 모델 평가를 진행해보았는데, 더 많은 데이터가 원본 정의역에 항상 도움이 된 것을 확인
- 대상 정의역에서도 마찬가지로 데이터가 많을 수록 성능이 좋았음

2) 원본/대상 정의역 유사도 
- 전이 학습 애플리케이션의 원본 정의역 및 대상 정의역이 매우 다를 경우 유사한 경우보다 더 많은 데이터가 필요
- 분류해야 하는 문제가 시각적으로 매우 다른 경우, 특징 추출 계층들이 학습을 아주 많이 다시 해야 했기 때문에 미세 조정도 더 많이 해야 했음 


4. 대상 정의역 
- 캐글이 제공하는 개/고양이 데이터셋을 이용한 이진 분류 문제 
- 25,000개에 이르는 개/고양이 이미지로 구성되어 있으며, 각각 12,5000개씩 한 클래스를 구성하고 있음 
- https://www.kaggle.com/c/dogs-vs-cats/data에서 다운로드 가능 
- 각 사진에 개 또는 고양이가 들어 있지만, 둘 다 들어있지는 않음 


5. 원본 정의역 
- 이미지넷으로 훈련한 심층 신경망 
- 개와 고양이의 이미지가 이미지넷 데이터셋에도 모두 있음 

1) 원본 신경망 아키텍처
- Inception-V3 신경망 아키텍처 (https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf) 사용
- 해당 아키텍처는 인셉션 모듈이라고 하는 빌딩 블록을 기반으로 하며, 각 인셉션 모듈에서 이전의 활성치가 1*1 합성곱, 3*3 합성곱, 5*5 합성곱 및 최대 풀링 계층에 부여되며
  이후 출력이 함께 연결됨
- 해당 신경망은 여러 초기화 모듈로 구성되어 있으며, 마지막 두 계층은 모두 완전히 연결되어 있고 출력 계층에는 softmax 활성을 지닌 1,000개 뉴런이 있음
- 케라스는 keras.applications 안에 자리잡고 있는 '신경망 동물원'에 여러 인기있는 신경망을 두었으며, 여기서는 이를 활용함 

$ from keras.applications.inception_v3 import InceptionV3  
$ 
$ base_model = InceptionV3(weights='imagenet', include_top=False)

-> 'include_top=False' option을 사용하면 신경망의 최상위 계층을 우리가 원하지 않는다는 뜻으로, 직접 그것들을 제거할 여지를 남길 수 있음 
   해당 코드를 통해 Inception-V3 신경망 아키텍처를 내려받아 가중치를 저장하고, 임시저장(cache)


2) 전이 신경망 아키텍처
- 최종 두 계층을 사용 사례에 더 적합한 완전 연결 계층으로 교체해야 함 

$ x = base_model.output   
$ x = GlobalAveragePooling2D()(x)
$ x = Dense(1024, activation='relu')(x) 
$ predictions = Dense(1, activation='sigmoid')(x) 

$ model = Model(inputs=base_model.input, outputs=predictions)

-> GlobalAveragePooling Layer를 통해 이전 계층의 4차원 출력을 2차원 계층으로 평탄화하고, 이후 완전 연결 계층과 연결

3) 기초 모델의 계층들 동결
- 새로운 완전 연결 계층들이 학습될 때 기존 가중치들이 변하지 않도록 기초 모델의 계층들을 동결시킴

$ for layer in base_model.layers:
$     layer.trainable = False 


6. 데이터 준비
1) 데이터 입력 
- 캐글에서 제공하는 개/고양이 이미지의 해상도와 크기가 모두 달라, 이에 대한 전처리를 수행해 동일하게 통일해 주어야 함 
- Incpetion-V3의 input image size: 299 * 299 * 3 

$ train_datagen = ImageDataGenerator(rescale=1./255)
$ val_datagen = ImageDataGenerator(rescale=1./255)

$ train_generator = train_datagen.flow_from_directory(train_data_dir, 
$     target_size=(img_width, img_height),
$     batch_size=batch_size, 
$     class_mode='binary')    

$ validation_generator = val_datagen.flow_from_directory(val_data_dir,
$     target_size=(img_width, img_height),
$     batch_size=batch_size,
$     class_mode='binary')

-> 데이터 생성기의 flow_from_directory() 메서드를 활용하여 경로를 지정받아 해당 경로를 통해 이미지 배치를 생성하도록 하였음 
   이와 같이 할 경우 디스크에서 이미지를 끌어 오는 모든 작업을 자동으로 수행하며, 이러한 작업을 한 번에 1배치씩 처리


7. 모델 훈련 (특징 추출)
- 신경망을 동결한 상태에서 10 epoch에 걸친 훈련을 통해 특징을 추출함 
