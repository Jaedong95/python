# 딥러닝에 관한 기본 개념과, 딥러닝을 이용한 회귀 문제 풀이, 텐서보드 사용 방법에 대해 다룸 

''' 딥러닝 기본 지식, 2022.02.20 ''' 

0. 심층 신경망 
- 은닉 계층이 두 개 이상인 신경망으로 정의 
asds
1. 뉴런이란 ? 
- 신경망의 최소 단위로, 뉴런의 핵심은 대부분 함께 작용하는 함수 두 개로 이루어져 있다는 것
- 두 개의 함수는 선형 함수, 활성화 함수에 해당

1) 선형 함수 
- 각 입력치에 계수(가중치)를 곱한 다음에 이것을 출력치로 삼는 선형 함수 
- x1, x2, x3이라는 입력 특징과 z라는 출력이 있는 일부 뉴런이 주어졌을 때, 

z = x1*a1 + x2*a2 + x3*a3 + b 

- {a1, a2, ... , an}: 데이터에 부여하는 가중치(계수), b: 편향치를 나타냄

2) 활성화 함수 
- 뉴런들 사이에 비선형성을 도입하는데 사용되는 함수 
- 활성화 함수를 도입하지 않을 경우 선형 활성(z=z와 같이 실제로는 활성화가 안되는 함수)들이 있는 뉴런들로 인해 바람직한 모델링이 이루어지지 않는다.
- 선형 함수로 모델링을 할 수 없는 함수의 표준 사례로는 '배타적 논리합 함수'를 들 수 있음 
- sigmoid, relu, tanh 등이 대표적인 활성화 함수 


2. 손실 함수와 비용 함수 
1) 비용 함수
- 모든 머신러닝 모델은 비용 함수를 가지고 시작됨
- 비용 함수를 사용하면 모델이 훈련 데이터에 얼마나 더 잘 적합한지를 측정할 수 있음
- 비욤 함수: 훈련 집합 전체의 손실을 평균화 (손실 함수의 평균 = 비용 함수) 

2) 손실 함수
- 훈련 집합 내의 단일 관측이 얼마나 적합한지를 나타내는 정확도 
- 제곱오차: 손실 함수, 평균제곱오차: 비용 함수라고 볼 수 있음 


3. 순전파와 역전파 
1) 순전파 
- 한 차례 관측만으로 나타난 특징들을 사용해 표적 변수를 예측하려고 하는 과정

  w1 * x + b -> f(x)=max(0, x) -> w2*x + b 

2) 역전파
- 순전파가 완료되면 각 데이터 점에 대한 신경망 예측이 제공되며, 이에 데이터 점의 실제값과 데이터 점의 예측 값을 이용해 신경망의 오차를 계산 할 수 있음
- 신경망에서 학습이 이루어지려면, 신경망의 오차 신호가 마지막 계층에서 첫 번째 계층에 이르기까지 신경망 계층을 거치며 역방향으로 전파되어야 함 
- 신호를 옮기면서 신경망 오차를 갱신하는 동안 이 오차 신호를 역방향으로 전파하는 것이 역전파 .. 
- 비용 함수를 가장 작게 만드는 값으로 향하도록 가중치를 조정하는 방식으로 비용 함수를 최소가 되게 해야 하는데, 이 과정을 '경사 하강'이라고 함 

4. 경사 하강 
- 경사: 신경망 내의 각 가중치에 대한 오차 함수의 편미분 
- 연쇄 법칙과 위쪽 계층의 경사를 사용해 각 가중치의 경사를 계층별로 계산할 수 있음
- 각 계층의 경사가 알려지면, 경사 하강 알고리즘을 사용해 비용 함수를 최소화 할 수 있음
- 경사 하강 알고리즘은 신경망의 오차가 최소화되고 과정이 수렴될 때까지 해당 갱신을 반복함

* 경사 하강 알고리즘: 학습 속도(알파)에 맞추어 경사도를 곱하고, 각 가중치의 현재 값에서 해당 경사 값을 뺌 (학습 속도: 하이퍼파라미터)

1) 미니배치 경사 하강 
- 전체 데이터셋과 관련해 순방향 전달을 가정하거나 이에 상응하는 역방향 전달을 가정하는 '배치 경사 하강'과 반대되는 개념
- 데이터의 무작위 부분 집합(미니 배치)을 가져와서 오차를 계산하고, 이를 이용해 신경망 가중치를 갱신함 

2) 단일 점의 오차 추정
- 한 번에 하나의 데이터 점을 사용해 신경망의 가중치를 갱신하는 것
- 수렴 속도를 높이는 데 도움이 될 수 있지만, 이는 전체 데이터셋의 좋은 근사가 아닐 수도 있음 


5. 딥러닝을 위한 최적화 알고리즘
1) 경사 하강 시 운동량(momentum) 사용 
- 경사가 한 방향으로 이어질 때는 학습 속도를 높이고, 경사가 오르내리는 경향이 있을 때는 학습을 느리게 함으로써 경사 하강을 가속화 할 수 있음
- 운동량: 속도 항을 도입하고, 갱신 규칙에서 해당 항의 가중 이동 평균을 사용하는 방식으로 계산 
- 보통 B(베타)값을 0.9로 설정하며, 대개 운동량은 하이퍼파라미터가 아님

2) RMSProp 
- 신경망 가중치가 나타내는 다차원 공간에서 다른 방향으로 이리저리 움직이는 일을 완화하는 한편, 
  어느 한 쪽 방향에 대한 학습 속도는 높임으로써 경사 하강 속도를 높이는 데 사용 가능 

3) Adam 최적화기
- 가장 잘 알려진 최적화기 중 하나로, 운동량과 RMSProp의 장점을 하나의 갱신 규칙으로 결합하여 사용 

6. 편향 오차와 분산 오차
1) 편향(bias) 오차
- 모델이 도입한 오차로, 선형 모델을 사용해 비선형 함수를 모델링하려고 시도하면 모델이 특정되기 어렵고 편향이 높아짐

2) 분산(variance) 오차 
- 학습 데이터의 임의성에 의해 생기는 오차
- 훈련 분포에 적합될 때 모델은 더 이상 일반화되지 않을 가능성이 크며, 과적합되거나 분산 오차 발생 

* 가능한 적은 분산 오차를 발생시키며 편향 오차를 최소화하는 절충안을 찾아야 함 


7. 심층 신경망의 편향과 분산 관리 
1) 큰 편향(high bias)
- 편향이 큰 신경망은 훈련 집합을 예측할 때 오차율이 매우 높아, 데이터를 적합하게 하는 일을 잘 하지 못함
- 편향을 줄이기 위해서 신경망 아키텍처를 변경하거나, 계층이나 뉴런을 추가하는 과정이 필요 
- 합성곱 신경망이나 재귀 신경망을 사용하는 방법도 고려 

2) 큰 분산(high variance)
- 신경망의 검증 오차가 테스트 오차보다 큰 경우에는 신경망이 훈련 데이터에 과적합되기 시작함 
- 분산을 줄이기 위해 데이터를 신경망에 추가하거나, 정칙화(regularization)을 추가할 수 있음

* 일반적인 정칙화 기법: L2 정칙화, 드롭아웃(dropout), 배치 정규화(batch normalization) 


''' 딥러닝으로 회귀 문제 풀기, 2022.02.21 ''' 
1. 회귀 분석 
1) 고전적인 회귀 분석 
- 선형 모델을 사용해 독립 변수 집합과 종속 변수 집합 간의 관계 학습 
- 다른 모든 독립 변수가 일정하게 유지될 때, 단일 독립 변수가 종속 변수에 미치는 영향을 이해하기 위해 회귀 분석을 수행함 
- 전통적인 다중 선형 회귀 분석에서 가장 좋은 점 중 하나: '세테리스 패러버스 특성' 

* 세테리스 패러버스 특성이란 ? 
-> 독립 변수가 여러 개일 때 그 중 한 개를 제외한 나머지 독립 변수를 동일하게 유지하는 상태에서 한 가지 독립 변수로 모델을 만드는 데 필요한 특성 

2) 신경망을 사용한 회귀 분석 
- 특징을 선택하거나 거르지 않아도 됨 (장점)
- 적절하게 복잡한 신경망이 주어졌을 때 특징 간의 상호작용도 학습될 수 있음 (고위 다항식 관계 학습 가능) 
- 신경망을 해석하기 쉽지 않음 (단점)
- 특징과 데이터가 많아야 신경망이 가장 잘 작동함 (단점) 


2. 심층 신경망 구축 단계 
1) 해결하려는 문제를 개략적으로 설명
2) 모델의 입력 및 출력을 식별 
3) 비용 함수 및 계량(metrics) 선택
4) 초기 신경망 아키텍처 생성
5) 신경망을 훈련하고 조절 


3. StandardScaler를 이용한 데이터 정규화
- 데이터를 정규화함으로써, 각 특징의 눈금이 서로 비교할 수 있게 됨 
- 이러한 눈금이 모두 활성 함수가 처리할 수 있는 범위 내에 있도록 하는게 매우 중요

$ from sklearn.preprocessing import StandardScaler

$ scaler = StandardScaler()
$ train = scaler.fit_transform(train) 

* val = scaler.transform(val) 

4. dict를 이용한 데이터 구조화
$ data = dict()
$ data['train_x'] = train[:, 0:10]
$ data['train_y'] = train[:, 10]

... 

$ data['scaler'] = scaler 


5. 비용 함수 정의
- 회귀 분석을 할 떄 가장 널리 사용하는 비용 함수는 제곱근 평균 제곱 오차(RMSE), 평균 절대 오차(MAE)가 있음

1) MAE 
- 데이터셋의 모든 사례에 대해 평균적이고 부호 없음 (unsigned)

2) RMSE 
- 절대값 대신 평균 제곱 오차의 제곱근 사용 

* 오차가 데이터셋 전체에 고르게 분포되는 경우 RMSE와 MAE는 동일하지만, 데이터셋에 매우 큰 이상치가 있는 경우 RMSE는 MAE보다 훨씬 큼 


6. MLP 계층 구축 (입력, 은닉, 출력)
1) 입력 계층
- 입력 행렬의 행 개수: 데이터셋의 데이터 원소/관측 개수 
- 입력 행렬의 열 개수: 변수/특징 개수

  입력 행렬의 모양 = (관측 개수 * 10) 

* tensorflow 및 keras에서는 미니 배치에 들어 있는 레코드의 정확한 개수를 정의하는 대신 None을 자리 표시자로 사용할 수 있음

2) 은닉 계층
- 32개 뉴런으로 시작 
- 해당 뉴런 개수는 하이퍼파라미터로, 나중에 탐색해 조율 가능
- (10, 32) 형태 

3) 출력 계층 
- 은닉 계층으로부터 나오는 32개 값을 입력으로 사용해 각 관측에 대한 단일 출력 값을 예측하는 단일 뉴런으로 구성
- (32, 1) 형태 


7. model 최적화기 수정 
- 사용자 정의 Adam 인스턴스를 정의해 대신 사용하면 학습 속도 등을 조정할 수 있음

$ from keras.optimizers import Adam

$ adam_optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
$ model.compile(optimizer=adam_optimizer, loss='mean_absolute_error') 


8. model 학습
$ model.fit(x=data['train_x'], y=data['train_y'], batch_size=32, epochs=200, verbose=1, validation_data=(data['val_x'], data['val_y']))

- 케라스에서 기본 batch size = 32로, 이는 케라스가 사용할 미니배치의 크기를 나타냄 (미니배치를 사용하지 않을 경우 batch size=None으로 지정) 
- verbose를 2로 지정하면 미니배치마다 손실 정보를 출력 


9. model 성능 측정 
- train dataset의 MAE 값과 val dataset의 MAE 값을 비교하여 두 오차가 얼만큼 차이나는지(분산)에 따라 과적합 여부 판단 
- 훈련 오차와 검증 오차의 차이가 클 수록 분산이 높다는 뜻이고, 이는 훈련 집합에 과적합하다는 뜻으로 볼 수 있음 

* 모델이 많은 편향을 가지고 있을 경우에는 모델의 계층이나 뉴런 중 하나를 늘리거나 아니면 둘 다 늘림으로써 편향을 줄일 수 있음

ex) 심층 신경망을 사용하여 모델 학습 시 train MAE: 0.075, MLP를 이용한 모델 학습 시 train MAE: 0.19이면 심층 신경망을 사용할 경우 편향이 줄어든다는 것을 알 수 있음 

* 분산을 줄이는 간단한 방법은 훈련 데이터를 추가하거나, L2 정규화 혹은 드롭아웃과 같은 정규화 기법 적용 


10. model의 하이퍼파라미터 조율
- 은닉 계층의 개수를 실험 
- 각 은닉 계층의 뉴런 개수를 계층 개수와 비교하며 실험
- 드롭아웃과 정직화를 보태어 실험
- Adam -> SGD나 RMSProp을 써보거나 Adam을 사용하되 학습 속도를 다르게 해서 실험 


11. 훈련된 model을 저장하고 적재 
$ model.save('regression_model.h5')

$ from keras.models import load_model
$ model = load_model('regression_model.h5') 

* load_model시 str ~ decode error가 뜨는 경우
-> $ pip3 install 'h5py==2.10.0' --force-reinstall


''' 텐서보드로 신경망 훈련 과정 살펴보기, 2022.02.21 ''' 
1. 텐서보드란 ?
- 텐서플로를 사용해 생성한 심층 신경망의 계량이나 파라미터 및 구조를 시각화하는데 도움이 되는 웹 기반 애플리케이션 
- 텐서보드를 이용하면 신경망을 훈련할 때 계량과 파라미터의 변화를 살펴볼 수 있으므로 문제 해결 시간을 크게 단축시킬 수 있음 
- 독립형 웹 애플리케이션으로, 웹 브라우저를 거쳐야 텐서보드를 사용할 수 있음 


2. 텐서보드 설정 
1) 텐서보드 설치 
$ pip3 install -U tensorboard 

2) 로그 디렉토리 생성
- 텐서보드 및 텐서플로는 공통 로그 디렉토리를 사용해 정보를 공유함 

$ mkdir ~/ch3_tb_log

3) 텐서보드 실행
$ tensorboard --logdir ./ch3_tb_log/ --port 6006

6006 포트가 기본 값이지만, 다른 포트 번호도 사용 가능. http://<ip주소>:6006을 입력하면 텐서보드를 시작할 수 있음 

4) Keras <-> 텐서보드 연결 
- 이 작업을 하기 전에 콜백에 대해 먼저 알아야 함


3. 콜백(Callback)
- 훈련 과정 중에 실행할 수 있는 함수
- 1 epoch 후에 모델 가중치를 저장하거나, 로그를 기록하거나, 하이퍼파라미터를 변경하거나, 텐서보드 로그 파일을 작성할 수 있음 
- 심층 신경망과 함께 사용하고자 하는 콜백 목록을 만들기만 하면, 해당 목록을 모델의 fit() 함수의 인자로 전달 가능 

1) 텐서보드 콜백 생성 
$ from tensorflow.keras.callbacks import TensorBoard

$ def create_callbacks():
$     tensorboard_callback = TensorBoard(log_dir='./ch3_tb_log/mlp', histogram_freq=1, batch_size=32, 
$                                        write_graph=True, write_grads=False)
$     return [tensorboard_callback]

-> write_graph를 통해 텐서보드에서 신경망 그래프를 시각화 할 수 있고, write_grads를 통해 텐서보드에 경사에 관한 히스토그램을 계산할 수 있음

2) 모델 학습 
$ callbacks = create_callbacks()

$ model.fit(x=data['train_x'], y=data['train_y'], batch_size=32, epochs=200, verbose=1,
$           validation_data=(data['val_x'], data['val_y']), callbacks=callbacks)

학습이 완료되면 지정한 로그 디렉토리에 로그 파일이 생성된 것을 확인할 수 있다. 텐서보드를 살펴보자.


4. 텐서보드 사용 
- X축의 기본 눈금: epochs
- y축의 값: MAX인 손실 함수 
- DISTRIBUTIONS 탭: 가중치와 편향 비교 가능
- HISTOGRAMS 탭: 은닉 계층 등 시각화 가능 


5. 신경망 깨트리기 (문제 발생시키기) 
- 우리는 신경망이 언제 손상될 지 전혀 알지 못함 
- 일부러 신경망을 깨드리기 위한 간단한 방법은, 신경망의 모든 뉴런을 아주 똑같은 값으로 초기화하면 됨 
- 신경망의 모든 뉴런을 똑같은 값으로 초기화 할 경우, 역전파가 되는 동안 모든 뉴런이 받는 오차가 아주 똑같고 정확히 같은 방식으로 변하기 때문에 
  신경망이 대칭을 깨지 못함 







