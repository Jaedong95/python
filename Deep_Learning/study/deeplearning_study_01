''' 딥러닝 기본 지식, 2022.02.20 ''' 

0. 심층 신경망 
- 은닉 계층이 두 개 이상인 신경망으로 정의 

1. 뉴런이란 ? 
- 신경망의 최소 단위로, 뉴런의 핵심은 대부분 함께 작용하는 함수 두 개로 이루어져 있다는 것
- 두 개의 함수는 선형 함수, 활성화 함수에 해당

1) 선형 함수 
- 각 입력치에 계수(가중치)를 곱한 다음에 이것을 출력치로 삼는 선형 함수 
- x1, x2, x3이라는 입력 특징과 z라는 출력이 있는 일부 뉴런이 주어졌을 때, 

z = x1*a1 + x2*a2 + x3*a3 + b 

- {a1, a2, ... , an}: 데이터에 부여하는 가중치(계수), b: 편향치를 나타냄

2) 활성화 함수 
- 뉴런들 사이에 비선형성을 도입하는데 사용되는 함수 
- 활성화 함수를 도입하지 않을 경우 선형 활성(z=z와 같이 실제로는 활성화가 안되는 함수)들이 있는 뉴런들로 인해 바람직한 모델링이 이루어지지 않는다.
- 선형 함수로 모델링을 할 수 없는 함수의 표준 사례로는 '배타적 논리합 함수'를 들 수 있음 
- sigmoid, relu, tanh 등이 대표적인 활성화 함수 


2. 손실 함수와 비용 함수 
1) 비용 함수
- 모든 머신러닝 모델은 비용 함수를 가지고 시작됨
- 비용 함수를 사용하면 모델이 훈련 데이터에 얼마나 더 잘 적합한지를 측정할 수 있음
- 비욤 함수: 훈련 집합 전체의 손실을 평균화 (손실 함수의 평균 = 비용 함수) 

2) 손실 함수
- 훈련 집합 내의 단일 관측이 얼마나 적합한지를 나타내는 정확도 
- 제곱오차: 손실 함수, 평균제곱오차: 비용 함수라고 볼 수 있음 


3. 순전파와 역전파 
1) 순전파 
- 한 차례 관측만으로 나타난 특징들을 사용해 표적 변수를 예측하려고 하는 과정

  w1 * x + b -> f(x)=max(0, x) -> w2*x + b 

2) 역전파
- 순전파가 완료되면 각 데이터 점에 대한 신경망 예측이 제공되며, 이에 데이터 점의 실제값과 데이터 점의 예측 값을 이용해 신경망의 오차를 계산 할 수 있음
- 신경망에서 학습이 이루어지려면, 신경망의 오차 신호가 마지막 계층에서 첫 번째 계층에 이르기까지 신경망 계층을 거치며 역방향으로 전파되어야 함 
- 신호를 옮기면서 신경망 오차를 갱신하는 동안 이 오차 신호를 역방향으로 전파하는 것이 역전파 .. 
- 비용 함수를 가장 작게 만드는 값으로 향하도록 가중치를 조정하는 방식으로 비용 함수를 최소가 되게 해야 하는데, 이 과정을 '경사 하강'이라고 함 

4. 경사 하강 
- 경사: 신경망 내의 각 가중치에 대한 오차 함수의 편미분 
- 연쇄 법칙과 위쪽 계층의 경사를 사용해 각 가중치의 경사를 계층별로 계산할 수 있음
- 각 계층의 경사가 알려지면, 경사 하강 알고리즘을 사용해 비용 함수를 최소화 할 수 있음
- 경사 하강 알고리즘은 신경망의 오차가 최소화되고 과정이 수렴될 때까지 해당 갱신을 반복함

* 경사 하강 알고리즘: 학습 속도(알파)에 맞추어 경사도를 곱하고, 각 가중치의 현재 값에서 해당 경사 값을 뺌 (학습 속도: 하이퍼파라미터)

1) 미니배치 경사 하강 
- 전체 데이터셋과 관련해 순방향 전달을 가정하거나 이에 상응하는 역방향 전달을 가정하는 '배치 경사 하강'과 반대되는 개념
- 데이터의 무작위 부분 집합(미니 배치)을 가져와서 오차를 계산하고, 이를 이용해 신경망 가중치를 갱신함 

2) 단일 점의 오차 추정
- 한 번에 하나의 데이터 점을 사용해 신경망의 가중치를 갱신하는 것
- 수렴 속도를 높이는 데 도움이 될 수 있지만, 이는 전체 데이터셋의 좋은 근사가 아닐 수도 있음 


5. 딥러닝을 위한 최적화 알고리즘
1) 경사 하강 시 운동량(momentum) 사용 
- 경사가 한 방향으로 이어질 때는 학습 속도를 높이고, 경사가 오르내리는 경향이 있을 때는 학습을 느리게 함으로써 경사 하강을 가속화 할 수 있음
- 운동량: 속도 항을 도입하고, 갱신 규칙에서 해당 항의 가중 이동 평균을 사용하는 방식으로 계산 
- 보통 B(베타)값을 0.9로 설정하며, 대개 운동량은 하이퍼파라미터가 아님

2) RMSProp 
- 신경망 가중치가 나타내는 다차원 공간에서 다른 방향으로 이리저리 움직이는 일을 완화하는 한편, 
  어느 한 쪽 방향에 대한 학습 속도는 높임으로써 경사 하강 속도를 높이는 데 사용 가능 

3) Adam 최적화기
- 가장 잘 알려진 최적화기 중 하나로, 운동량과 RMSProp의 장점을 하나의 갱신 규칙으로 결합하여 사용 

6. 편향 오차와 분산 오차
1) 편향(bias) 오차
- 모델이 도입한 오차로, 선형 모델을 사용해 비선형 함수를 모델링하려고 시도하면 모델이 특정되기 어렵고 편향이 높아짐

2) 분산(variance) 오차 
- 학습 데이터의 임의성에 의해 생기는 오차
- 훈련 분포에 적합될 때 모델은 더 이상 일반화되지 않을 가능성이 크며, 과적합되거나 분산 오차 발생 

* 가능한 적은 분산 오차를 발생시키며 편향 오차를 최소화하는 절충안을 찾아야 함 


7. 심층 신경망의 편향과 분산 관리 
1) 큰 편향(high bias)
- 편향이 큰 신경망은 훈련 집합을 예측할 때 오차율이 매우 높아, 데이터를 적합하게 하는 일을 잘 하지 못함
- 편향을 줄이기 위해서 신경망 아키텍처를 변경하거나, 계층이나 뉴런을 추가하는 과정이 필요 
- 합성곱 신경망이나 재귀 신경망을 사용하는 방법도 고려 

2) 큰 분산(high variance)
- 신경망의 검증 오차가 테스트 오차보다 큰 경우에는 신경망이 훈련 데이터에 과적합되기 시작함 
- 분산을 줄이기 위해 데이터를 신경망에 추가하거나, 정칙화(regularization)을 추가할 수 있음

* 일반적인 정칙화 기법: L2 정칙화, 드롭아웃(dropout), 배치 정규화(batch normalization) 


''' 딥러닝으로 회귀 문제 풀기, 2022.02.21 ''' 
1. 회귀 분석 
1) 고전적인 회귀 분석 
- 선형 모델을 사용해 독립 변수 집합과 종속 변수 집합 간의 관계 학습 
- 다른 모든 독립 변수가 일정하게 유지될 때, 단일 독립 변수가 종속 변수에 미치는 영향을 이해하기 위해 회귀 분석을 수행함 
- 전통적인 다중 선형 회귀 분석에서 가장 좋은 점 중 하나: '세테리스 패러버스 특성' 

* 세테리스 패러버스 특성이란 ? 
-> 독립 변수가 여러 개일 때 그 중 한 개를 제외한 나머지 독립 변수를 동일하게 유지하는 상태에서 한 가지 독립 변수로 모델을 만드는 데 필요한 특성 

2) 신경망을 사용한 회귀 분석 
- 특징을 선택하거나 거르지 않아도 됨 (장점)
- 적절하게 복잡한 신경망이 주어졌을 때 특징 간의 상호작용도 학습될 수 있음 (고위 다항식 관계 학습 가능) 
- 신경망을 해석하기 쉽지 않음 (단점)
- 특징과 데이터가 많아야 신경망이 가장 잘 작동함 (단점) 


2. 심층 신경망 구축 단계 
1) 해결하려는 문제를 개략적으로 설명
2) 모델의 입력 및 출력을 식별 
3) 비용 함수 및 계량(metrics) 선택
4) 초기 신경망 아키텍처 생성
5) 신경망을 훈련하고 조절 


3. StandardScaler를 이용한 데이터 정규화
- 데이터를 정규화함으로써, 각 특징의 눈금이 서로 비교할 수 있게 됨 
- 이러한 눈금이 모두 활성 함수가 처리할 수 있는 범위 내에 있도록 하는게 매우 중요

$ from sklearn.preprocessing import StandardScaler

$ scaler = StandardScaler()
$ train = scaler.fit_transform(train) 


4. dict를 이용한 데이터 구조화
$ data = dict()
$ data['train_x'] = train[:, 0:9]
$ data['train_y'] = train[:, 10]

... 

$ data['scaler'] = scaler 


