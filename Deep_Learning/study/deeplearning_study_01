''' 뉴런의 개념 및 ''' 

0. 심층 신경망 
- 은닉 계층이 두 개 이상인 신경망으로 정의 

1. 뉴런이란 ? 
- 신경망의 최소 단위로, 뉴런의 핵심은 대부분 함께 작용하는 함수 두 개로 이루어져 있다는 것
- 두 개의 함수는 선형 함수, 활성화 함수에 해당

1) 선형 함수 
- 각 입력치에 계수(가중치)를 곱한 다음에 이것을 출력치로 삼는 선형 함수 
- x1, x2, x3이라는 입력 특징과 z라는 출력이 있는 일부 뉴런이 주어졌을 때, 

z = x1*a1 + x2*a2 + x3*a3 + b 

- {a1, a2, ... , an}: 데이터에 부여하는 가중치(계수), b: 편향치를 나타냄

2) 활성화 함수 
- 뉴런들 사이에 비선형성을 도입하는데 사용되는 함수 
- 활성화 함수를 도입하지 않을 경우 선형 활성(z=z와 같이 실제로는 활성화가 안되는 함수)들이 있는 뉴런들로 인해 바람직한 모델링이 이루어지지 않는다.
- 선형 함수로 모델링을 할 수 없는 함수의 표준 사례로는 '배타적 논리합 함수'를 들 수 있음 
- sigmoid, relu, tanh 등이 대표적인 활성화 함수 


2. 손실 함수와 비용 함수 
1) 비용 함수
- 모든 머신러닝 모델은 비용 함수를 가지고 시작됨
- 비용 함수를 사용하면 모델이 훈련 데이터에 얼마나 더 잘 적합한지를 측정할 수 있음
- 비욤 함수: 훈련 집합 전체의 손실을 평균화 (손실 함수의 평균 = 비용 함수) 

2) 손실 함수
- 훈련 집합 내의 단일 관측이 얼마나 적합한지를 나타내는 정확도 
- 제곱오차: 손실 함수, 평균제곱오차: 비용 함수라고 볼 수 있음 


3. 순전파와 역전파 
1) 순전파 
- 한 차례 관측만으로 나타난 특징들을 사용해 표적 변수를 예측하려고 하는 과정

  w1 * x + b -> f(x)=max(0, x) -> w2*x + b 

2) 역전파
- 순전파가 완료되면 각 데이터 점에 대한 신경망 예측이 제공되며, 이에 데이터 점의 실제값과 데이터 점의 예측 값을 이용해 신경망의 오차를 계산 할 수 있음
- 신경망에서 학습이 이루어지려면, 신경망의 오차 신호가 마지막 계층에서 첫 번째 계층에 이르기까지 신경망 계층을 거치며 역방향으로 전파되어야 함 
- 신호를 옮기면서 신경망 오차를 갱신하는 동안 이 오차 신호를 역방향으로 전파하는 것이 역전파 .. 
- 비용 함수를 가장 작게 만드는 값으로 향하도록 가중치를 조정하는 방식으로 비용 함수를 최소가 되게 해야 하는데, 이 과정을 '경사 하강'이라고 함 

4. 경사 하강 
- 경사: 신경망 내의 각 가중치에 대한 오차 함수의 편미분 
- 연쇄 법칙과 위쪽 계층의 경사를 사용해 각 가중치의 경사를 계층별로 계산할 수 있음
- 각 계층의 경사가 알려지면, 경사 하강 알고리즘을 사용해 비용 함수를 최소화 할 수 있음
- 경사 하강 알고리즘은 신경망의 오차가 최소화되고 과정이 수렴될 때까지 해당 갱신을 반복함

* 경사 하강 알고리즘: 학습 속도(알파)에 맞추어 경사도를 곱하고, 각 가중치의 현재 값에서 해당 경사 값을 뺌 (학습 속도: 하이퍼파라미터)

1) 미니배치 경사 하강 
- 전체 데이터셋과 관련해 순방향 전달을 가정하거나 이에 상응하는 역방향 전달을 가정하는 '배치 경사 하강'과 반대되는 개념
- 데이터의 무작위 부분 집합(미니 배치)을 가져와서 오차를 계산하고, 이를 이용해 신경망 가중치를 갱신함 

2) 단일 점의 오차 추정
- 한 번에 하나의 데이터 점을 사용해 신경망의 가중치를 갱신하는 것
- 수렴 속도를 높이는 데 도움이 될 수 있지만, 이는 전체 데이터셋의 좋은 근사가 아닐 수도 있음 


5. 딥러닝을 위한 최적화 알고리즘
1) 경사 하강 시 운동량(momentum) 사용 
- 경사가 한 방향으로 이어질 때는 학습 속도를 높이고, 경사가 오르내리는 경향이 있을 때는 학습을 느리게 함으로써 경사 하강을 가속화 할 수 있음
- 운동량: 속도 항을 도입하고, 갱신 규칙에서 해당 항의 가중 이동 평균을 사용하는 방식으로 계산 
- 보통 B(베타)값을 0.9로 설정하며, 대개 운동량은 하이퍼파라미터가 아님

2) RMSProp 
- 신경망 가중치가 나타내는 다차원 공간에서 다른 방향으로 이리저리 움직이는 일을 완화하는 한편, 
  어느 한 쪽 방향에 대한 학습 속도는 높임으로써 경사 하강 속도를 높이는 데 사용 가능 

3) Adam 최적화기
- 가장 잘 알려진 최적화기 중 하나로, 운동량과 RMSProp의 장점을 하나의 갱신 규칙으로 결합하여 사용 

